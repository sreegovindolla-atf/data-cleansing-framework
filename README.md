# Data Cleansing & Extraction Framework

## Overview
This project is a modular **data extraction and post-processing framework** designed to:
- Extract structured data from unstructured or semi-structured text using AI-based extraction
- Store intermediate results as JSON / JSONL / CSV
- Post-process and normalize extracted data
- Load clean, structured data into relational database tables (SQL Server)

The framework is built to be **reproducible, auditable, and scalable**, making it suitable for data engineering, analytics, and reporting use cases.

---

## Architecture (High Level)

```text
Input Text / Documents
        â†“
AI Extraction (data_extraction.py)
        â†“
JSONL / JSON / CSV Outputs
        â†“
Post Processing (post_processing.py)
        â†“
Normalized SQL Tables
```
---

## Project Structure

```text
data-cleansing-framework/
â”‚
â”œâ”€â”€ src/
| â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ data_extraction.py # AI-based extraction logic
â”‚ â”œâ”€â”€ post_processing.py # Cleansing, normalization, DB writes
â”‚
â”œâ”€â”€ data/
| â””â”€â”€ input/
|   â””â”€â”€ denorm_mastertable.csv
â”‚ â””â”€â”€ outputs/
â”‚   â””â”€â”€ <run_id>/
â”‚     â”œâ”€â”€ <run_id>_combined_extraction_results.jsonl
â”‚     â”œâ”€â”€ <run_id>_combined_extraction.json
â”‚     â”œâ”€â”€ <run_id>_combined_extraction.csv
|     â””â”€â”€ <run_id>_lx_cache.pkl
â”‚
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ prompt.py
| â””â”€â”€ examples/
â”‚   â”œâ”€â”€ distribution_projects.py
|   â”œâ”€â”€ infrastructure_projects.py
|   â””â”€â”€ service_projects.py
|
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ extraction_helpers.py
| â”œâ”€â”€ post_processing_helpers.py
| â””â”€â”€ post_processing_sql_queries.py
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## Prerequisites

Install dependencies:
```text
pip install -r requirements.txt
```

---

## ğŸš€ How to Run

You can run the framework in **two ways**:
1. Run each step individually
2. Run the entire pipeline using a single entry point (`main.py`)

---

### 1ï¸âƒ£a Run Data Extraction (Standalone)

This step reads raw input text and produces structured extraction outputs.

```bash
python src/data_extraction.py --run-id <RUN_ID>
```
Generated outputs:

<run_id>_combined_extraction_results.jsonl

<run_id>_combined_extraction.json

<run_id>_combined_extraction.csv

### 1ï¸âƒ£b Run Post Processing (Standalone)

This step reads extracted JSON / JSONL / CSV files, cleans and normalizes fields, and writes data into multiple SQL Server tables

```bash
python src/post_processing.py --run-id <RUN_ID>
```

### 2ï¸âƒ£ Run End-to-End Pipeline (Recommended)
To run the entire workflow (extraction + post-processing) in one go, use the main entry point.
In this case, a run_id will be auto-generated using the current date and time
**Example auto-generated format:** 20251231_160609

```bash
python src/main.py
```

This will:
1. Execute data extraction
2. Generate intermediate output files (jsonl, json, csv)
3. Perform post-processing
4. Load cleaned data into SQL Server

---

## Database Design

The post-processing layer writes into separate normalized tables, for example:

Master projects table

Project details table

Project assets table

---

ğŸ‘¤ Author

Sree Spoorthy G

Data Engineer

---

ğŸ“œ License

This project is licensed under the MIT License.  
See the `LICENSE` file for details.

---